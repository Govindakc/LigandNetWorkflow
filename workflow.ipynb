{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ligandnet workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**************************************\n",
    "# Govinda KC                          #\n",
    "# UTEP, Computational Science         #\n",
    "# Last modified: 1/25/20              #\n",
    "# *************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [21:19:01] Enabling RDKit 2019.09.2 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os, sys, json, glob\n",
    "sys.path.append('utilities')\n",
    "from train2 import Train\n",
    "from fetch_ligand2 import Pharos_Data\n",
    "from utility import FeatureGenerator # for features generation of  txt file\n",
    "from utility2 import FeatureGenerator2 # for features generation of sdf file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import  make_scorer, roc_auc_score, recall_score, accuracy_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Run_Workflow:\n",
    "    def __init__(self, actives, decoys):\n",
    "        self.actives = actives\n",
    "        self.decoys = decoys\n",
    "        self.results = dict()\n",
    "        \n",
    "    def get_fingerprints(self,smiles):\n",
    "        try:\n",
    "            fg = FeatureGenerator(smiles)\n",
    "            features = fg.toTPATF()\n",
    "            return features\n",
    "        except Exception as e: print(e)\n",
    "    \n",
    "    def get_models(self):\n",
    "        # Get features at first!\n",
    "        if not self.fp_generation():\n",
    "            print('Error: features extraction failed!')\n",
    "            return\n",
    "        try:\n",
    "            t = Train(self.actives_x, self.decoys_x)\n",
    "            t.train_models()\n",
    "        except Exception as e: print(e)\n",
    "\n",
    "    def fp_generation(self):\n",
    "        # Fingerprint generation\n",
    "        print('Pleae wait! Fingerprints are getting generated......')\n",
    "        if self.decoys[-4:] == '.sdf' and self.actives[-4:] == '.sdf':\n",
    "            # Get fingerprints for actives\n",
    "            self.actives_x = self.sdf_fp_active()\n",
    "            # Get fingerprints for decoys\n",
    "            self.decoys_x = self.sdf_fp_decoy()\n",
    "            return True\n",
    "        elif self.decoys[-4:] == '.sdf':\n",
    "            df = pd.read_csv(self.actives)\n",
    "#             df = pd.read_csv(open(self.actives,'rU'))#, encoding='utf-8', engine='c')\n",
    "            # Get fingerprints for actives\n",
    "            df['tpatf'] = df.SMILES.apply(self.get_fingerprints)\n",
    "            self.actives_x = np.array([f for f in df.tpatf.values], dtype = np.float32)\n",
    "            # Get fingerprints for decoys\n",
    "            self.decoys_x = self.sdf_fp_decoy()\n",
    "            return True\n",
    "        else:\n",
    "            df = pd.read_csv(self.actives)\n",
    "            df2 = pd.read_csv(self.decoys)\n",
    "#             df = pd.read_csv(open(self.actives,'rU'))#, encoding='utf-8', engine='c')\n",
    "#             df2 = pd.read_csv(open(self.decoys, 'rU'))#, encoding='utf-8', engine='c')\n",
    "            # Get fingerprints for actives\n",
    "            df['tpatf'] = df.SMILES.apply(self.get_fingerprints)\n",
    "            # Get fingerprints for decoys\n",
    "            df2['tpatf'] = df2.SMILES.apply(self.get_fingerprints)\n",
    "            # numpy arrays\n",
    "            self.actives_x = np.array([f for f in df.tpatf.values], dtype = np.float32)\n",
    "            self.decoys_x = np.array([f for f in df2.tpatf.values], dtype = np.float32)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def sdf_fp_decoy(self):\n",
    "        try:\n",
    "            fg2 = FeatureGenerator2(self.decoys)\n",
    "            feat_decoy = fg2.sepTPATF()\n",
    "            return feat_decoy\n",
    "        except Exception as e: print(e) \n",
    "\n",
    "    def sdf_fp_active(self):\n",
    "        try:\n",
    "            fg2 = FeatureGenerator2(self.actives)\n",
    "            feat_active = fg2.sepTPATF()\n",
    "            return feat_active\n",
    "        except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If users have their own actives and decoys\n",
    "def actives_decoys():\n",
    "    active_file = input(\"Uniprot id of the file? Example: P07948  \\n\")   \n",
    "    active_file = active_file.strip()\n",
    "    print('Looking for active and decoy files....')\n",
    "    # active in .txt\n",
    "    actives = main_path+'actives/'+active_file+'.txt'\n",
    "    if not os.path.isfile(actives):\n",
    "        # active in .sdf\n",
    "        actives = main_path+'actives/'+active_file+'.sdf'\n",
    "    # decoy in .txt..\n",
    "    decoys = main_path+'decoys/'+\"decoys_\" + active_file +\".txt\"\n",
    "    if not os.path.isfile(decoys):\n",
    "        # decoy in .sdf..\n",
    "        decoys = main_path+'decoys/'+ \"decoys_\" +active_file+\".sdf\"\n",
    "    if os.path.isfile(actives) and os.path.isfile(decoys):\n",
    "        print('Actives and Decoys are found!')\n",
    "    return actives, decoys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searches decoys in our database for give active file (Uniprot id)\n",
    "def actives_bt_not_decoys():\n",
    "    active_file = input(\"Uniprot id of the file? Example: P07948 \\n\")\n",
    "    active_file = active_file.strip()\n",
    "    actives = main_path+'actives/'+active_file+'.txt'\n",
    "    if not os.path.isfile(actives):\n",
    "        actives = main_path+'actives/'+active_file+'.sdf'\n",
    "    # Path for decoys database\n",
    "    decoys_database = '../decoys_database'\n",
    "#     if not os.path.isfile(os.path.join(decoys_database, active_file+\".txt\")):\n",
    "    print('Searching decoys .....')\n",
    "    if not os.path.isfile(os.path.join(decoys_database, active_file+\".sdf\")):\n",
    "        print(\"Decoys are not found, exiting! Look for decoys in DUDE website and come back!\")\n",
    "        sys.exit(1)\n",
    "#     decoys = os.path.join(decoys_database, active_file+\".txt\")\n",
    "    decoys = os.path.join(decoys_database, \"decoys_\" +active_file+\".sdf\")\n",
    "    if os.path.isfile(actives) and os.path.isfile(decoys):\n",
    "        print('Actives and decoys are extracted!')\n",
    "    return actives, decoys\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_actives_and_decoys():\n",
    "    \n",
    "    active_file = input(\"Uniprot id of the file? Example: P07948 \\n\")\n",
    "    active_file = active_file.strip()\n",
    "    active_dir = main_path+'/'+ \"actives\"\n",
    "    pdata = Pharos_Data(active_file, active_dir )\n",
    "    print('Actives for a given protein are getting downloaded from Pharos website!')\n",
    "    pdata.fetch_ligand()\n",
    "    actives = main_path+'actives/'+active_file+'.txt'\n",
    "    print('Searching decoys .....')\n",
    "    decoys_database = '../decoys_database/'\n",
    "    if not os.path.isfile(os.path.join(decoys_database, \"decoys_\" +active_file+\".sdf\")):\n",
    "        print(\"Decoys are not found, exiting! Look for decoys in DUDE website and come back!\")\n",
    "        sys.exit(1)\n",
    "    decoys = os.path.join(decoys_database, active_file+\".sdf\")\n",
    "    if os.path.isfile(actives) and os.path.isfile(decoys):\n",
    "        print('Actives and decoys are extracted!')\n",
    "    return actives, decoys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, provide the path for working directory. Example: /Users/gvin/ligandnet_workflow/test_ligandnet/ \n",
      "\n",
      "/Users/gvin/ligandnet_workflow/test_ligandnet/\n",
      "Actives and decoys should either be in sdf file or text file (with header \"SMILES\" for txt files!)\n",
      "ACTIVES AND DECOYS FILE NAMES SHOULD BE LIKE THAT: P07948.txt(or .sdf) and decoys_P07948.txt (or .sdf) \n",
      "PLEASE, MAKE SURE YOU HAVE FOLDERS \"actives\" and \"decoys\"\n",
      "DO YOU HAVE \"actives\" and \"decoys\" FOLDERS? Type y for Yes and n for No!\n",
      "y\n",
      "Do you have actives? Please type y for Yes and n for No !\n",
      "y\n",
      "Do you have decoys? Please type y for Yes and n for No !\n",
      "y\n",
      "Uniprot id of the file? Example: P07948  \n",
      "P07948\n",
      "Looking for active and decoy files....\n",
      "Actives and Decoys are found!\n",
      "Pleae wait! Fingerprints are getting generated......\n",
      "Please choose the name (Example type 1 for Random Forest) of the model from the following options! \n",
      "1. Random Forest Classifier\n",
      "2. Extreme Gradient Boosting\n",
      "3. Support Vector Classifier\n",
      "4. Artificial Neural Network\n",
      "5. All\n",
      "6. Exit with out running any model\n",
      "2\n",
      "Training xgboost..\n",
      "Results:  {'xgb': {'roc_auc': 0.9576923076923077, 'accuracy': 0.9393939393939394, 'f1_score': 0.9393939393939394, 'cohen_kappa': 0.8730769230769231, 'mcc': 0.8730769230769231, 'data_info': {'train_count': 129, 'test_count': 33, 'actives_count': 62, 'decoys_count': 100}}}\n",
      "Writing results\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Start here\n",
    "def start_workflow():\n",
    "    \n",
    "    print('Actives and decoys should either be in sdf file or text file (with header \"SMILES\" for txt files!)')\n",
    "    print('ACTIVES AND DECOYS FILE NAMES SHOULD BE LIKE THAT: P07948.txt(or .sdf) and decoys_P07948.txt (or .sdf) ')\n",
    "    print('PLEASE, MAKE SURE YOU HAVE FOLDERS \"actives\" and \"decoys\"')\n",
    "    print('DO YOU HAVE \"actives\" and \"decoys\" FOLDERS? Type y for Yes and n for No!')\n",
    "    check = input()\n",
    "    if check != 'y':\n",
    "        print('Exiting...')\n",
    "        sys.exit(1)\n",
    "    print(\"Do you have actives? Please type y for Yes and n for No !\")\n",
    "    answer1 = input()\n",
    "          \n",
    "    print(\"Do you have decoys? Please type y for Yes and n for No !\")\n",
    "    answer2 = input()\n",
    "\n",
    "    if answer1 == 'y' and answer2 == 'y':\n",
    "        actives, decoys = actives_decoys()\n",
    "        rw = Run_Workflow(actives, decoys)\n",
    "        rw.get_models()\n",
    "    elif answer1 == 'y' and answer2 == 'n':\n",
    "        actives, decoys = actives_bt_not_decoys()\n",
    "        rw = Run_Workflow(actives, decoys)\n",
    "        rw.get_models()\n",
    "    elif answer1 == 'n' and answer2 == 'n':\n",
    "        actives, decoys = no_actives_and_decoys()\n",
    "        rw = Run_Workflow(actives, decoys)\n",
    "        rw.get_models()\n",
    "    else:\n",
    "        print('Please provide the right information!. Exiting!')\n",
    "        sys.exit(1)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    # Path for working directory\n",
    "    print(\"Please, provide the path for working directory. Example: /Users/gvin/ligandnet_workflow/test_ligandnet/ \\n\")\n",
    "    main_path = input()\n",
    "    main_path = main_path.strip()\n",
    "    os.chdir(main_path) \n",
    "    dirs = [\"actives\", \"decoys\"]\n",
    "    for _dir in dirs:\n",
    "        if not os.path.isdir(_dir): os.makedirs(_dir)\n",
    "    if main_path[-1]!='/':\n",
    "        main_path = main_path+'/'\n",
    "    # Start Function\n",
    "    start_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
